{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./shakespeare.txt\", mode=\"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citi\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# 1.generate vocabulary\n",
    "vocabulary = sorted(set(text))\n",
    "print(len(vocabulary))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "# 2.build mapping\n",
    "char2idx = {value:key for key, value in enumerate(vocabulary)}\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "# 根据索引找文字\n",
    "idx2char = np.array(vocabulary)\n",
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47]\n",
      "First Citi\n"
     ]
    }
   ],
   "source": [
    "# 3.transfer from char to index\n",
    "text_as_index = [char2idx[c] for c in text]\n",
    "\n",
    "print(len(text_as_index))\n",
    "print(text_as_index[:10])\n",
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_tensor_slices: 数组 =》 dataset\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(18, shape=(), dtype=int32) F\n",
      "tf.Tensor(47, shape=(), dtype=int32) i\n"
     ]
    }
   ],
   "source": [
    "for i in char_dataset.take(2):\n",
    "    print(i, idx2char[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch: 分成batch大小的dataset\n",
    "# batch_size:表示要在单个批次中合并的此数据集的连续元素个数\n",
    "# drop_remainder：表示在少于batch_size元素的情况下是否应删除最后一批\n",
    "\n",
    "seq_length = 100\n",
    "seq_dataset = char_dataset.batch(batch_size=seq_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11043"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1115394 / 101 = 11043\n",
    "len(seq_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59  1], shape=(101,), dtype=int32)\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "tf.Tensor(\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1 49], shape=(101,), dtype=int32)\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
     ]
    }
   ],
   "source": [
    "# 查看数据集元素\n",
    "for seq in seq_dataset.take(2):\n",
    "    print(seq)\n",
    "    # idx => char\n",
    "    print(repr(\"\".join([idx2char[index] for index in seq])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.预测文本 \n",
    "# data: abcd, return: abc,bcd\n",
    "def split_input_target(data):\n",
    "    return data[0:-1], data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59], shape=(100,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1\n",
      " 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
      " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6\n",
      "  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0\n",
      " 37 53 59  1], shape=(100,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1], shape=(100,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1 58\n",
      " 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0 13\n",
      " 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8  0\n",
      "  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1 63\n",
      " 53 59  1 49], shape=(100,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "seq_dataset = seq_dataset.map(split_input_target)\n",
    "\n",
    "# inputdata, outputdata: shape=100\n",
    "for inputdata, outputdata in seq_dataset.take(2):\n",
    "    print(inputdata)\n",
    "    print(outputdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 1000\n",
    "\n",
    "seq_dataset = seq_dataset.shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)\n",
    "\n",
    "# 11043/64 = 172\n",
    "print(len(seq_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, batch_size, units):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(\n",
    "            input_dim = vocab_size, \n",
    "            output_dim = embedding_dim, \n",
    "            # [batch_size, None]:元素个数确定， 其他未定\n",
    "            batch_input_shape = [batch_size, None]),\n",
    "        keras.layers.LSTM(\n",
    "            # units：输出空间的维度\n",
    "            # stateful：默认 False，如果为 True，则批次中索引 i 处的每个样品的最后状态将用作下一批次中索引 i 样品的初始状态。\n",
    "            # recurrent_initializer\n",
    "            # return_sequences：是返回输出序列中的最后一个输出，还是全部序列\n",
    "            units=units, \n",
    "            stateful = True,\n",
    "            recurrent_initializer = 'glorot_uniform',\n",
    "            return_sequences=True),\n",
    "        keras.layers.Dense(vocab_size),\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "batch_size = 64\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, batch_size, units)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[53 52  1 ... 56 47 52]\n",
      " [ 0 13 57 ... 25 63  1]\n",
      " [43 10  0 ... 58  1 58]\n",
      " ...\n",
      " [40 50 39 ... 39 56 58]\n",
      " [53 11  1 ... 46  1 46]\n",
      " [ 1 42 43 ...  1 55 59]], shape=(64, 100), dtype=int32) (64, 100)\n",
      "tf.Tensor(\n",
      "[[52  1 58 ... 47 52 45]\n",
      " [13 57  1 ... 63  1 52]\n",
      " [10  0 21 ...  1 58 56]\n",
      " ...\n",
      " [50 39 64 ... 56 58  1]\n",
      " [11  1 52 ...  1 46 47]\n",
      " [42 43 50 ... 55 59 53]], shape=(64, 100), dtype=int32) (64, 100)\n",
      "(64, 100, 65)\n"
     ]
    }
   ],
   "source": [
    "# 对于每个字符，模型会查找嵌入，把嵌入当作输入运行一个时间步，并用密集层生成逻辑回归 （logits），预测下一个字符的对数可能性。\n",
    "# 预测文本\n",
    "for input_example_batch, target_example_batch in seq_dataset.take(1):\n",
    "    print(input_example_batch, input_example_batch.shape)\n",
    "    print(target_example_batch, target_example_batch.shape)\n",
    "    # 函数式调用\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[19]\n",
      " [42]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [51]\n",
      " [52]\n",
      " [25]\n",
      " [36]\n",
      " [47]\n",
      " [41]\n",
      " [49]\n",
      " [52]\n",
      " [51]\n",
      " [48]\n",
      " [38]\n",
      " [27]\n",
      " [60]\n",
      " [11]\n",
      " [ 8]\n",
      " [ 4]\n",
      " [ 9]\n",
      " [42]\n",
      " [ 4]\n",
      " [18]\n",
      " [52]\n",
      " [51]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [57]\n",
      " [57]\n",
      " [11]\n",
      " [49]\n",
      " [58]\n",
      " [59]\n",
      " [24]\n",
      " [27]\n",
      " [55]\n",
      " [15]\n",
      " [12]\n",
      " [16]\n",
      " [19]\n",
      " [ 4]\n",
      " [56]\n",
      " [13]\n",
      " [49]\n",
      " [39]\n",
      " [ 6]\n",
      " [29]\n",
      " [26]\n",
      " [ 0]\n",
      " [37]\n",
      " [16]\n",
      " [19]\n",
      " [60]\n",
      " [11]\n",
      " [57]\n",
      " [26]\n",
      " [11]\n",
      " [10]\n",
      " [ 9]\n",
      " [38]\n",
      " [36]\n",
      " [62]\n",
      " [17]\n",
      " [13]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [30]\n",
      " [ 3]\n",
      " [54]\n",
      " [35]\n",
      " [38]\n",
      " [ 0]\n",
      " [44]\n",
      " [64]\n",
      " [57]\n",
      " [49]\n",
      " [ 6]\n",
      " [54]\n",
      " [53]\n",
      " [37]\n",
      " [ 9]\n",
      " [60]\n",
      " [36]\n",
      " [52]\n",
      " [ 8]\n",
      " [40]\n",
      " [52]\n",
      " [30]\n",
      " [ 6]\n",
      " [41]\n",
      " [46]\n",
      " [18]\n",
      " [43]\n",
      " [25]\n",
      " [32]\n",
      " [ 6]\n",
      " [12]\n",
      " [24]\n",
      " [34]], shape=(100, 1), dtype=int64)\n",
      "tf.Tensor(\n",
      "[19 42  4  6 51 52 25 36 47 41 49 52 51 48 38 27 60 11  8  4  9 42  4 18\n",
      " 52 51  6  9 57 57 11 49 58 59 24 27 55 15 12 16 19  4 56 13 49 39  6 29\n",
      " 26  0 37 16 19 60 11 57 26 11 10  9 38 36 62 17 13  8  9 30  3 54 35 38\n",
      "  0 44 64 57 49  6 54 53 37  9 60 36 52  8 40 52 30  6 41 46 18 43 25 32\n",
      "  6 12 24 34], shape=(100,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# tf.random.categorical：从一个分类分布中抽取样本\n",
    "# logits: 形状为 [batch_size, num_classes]的张量\n",
    "# num_samples：从每一行切片中抽取的独立样本的数量\n",
    "# [batch_size * num_classes] => [batch_size * num_samples] , [100, 65] => [100, 1]\n",
    "sample_indices = tf.random.categorical(logits=example_batch_predictions[0], num_samples=1)\n",
    "print(sample_indices)\n",
    "\n",
    "# 从input中删除一个纬度\n",
    "sample_indice = tf.squeeze(input=sample_indices, axis=1)\n",
    "print(sample_indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \"on thee; and thou art too full\\nOf the wars' surfeits, to go rove with one\\nThat's yet unbruised: brin\"\n",
      "output: \"n thee; and thou art too full\\nOf the wars' surfeits, to go rove with one\\nThat's yet unbruised: bring\"\n",
      "predict: 'Gd&,mnMXicknmjZOv;.&3d&Fnm,3ss;ktuLOqC?DG&rAka,QN\\nYDGv;sN;:3ZXxEA.3R$pWZ\\nfzsk,poY3vXn.bnR,chFeMT,?LV'\n"
     ]
    }
   ],
   "source": [
    "# 对比预测的数据\n",
    "print(\"input:\", repr(\"\".join([idx2char[i] for i in np.array(input_example_batch[0])])))\n",
    "print(\"output:\", repr(\"\".join([idx2char[i] for i in np.array(target_example_batch[0])])))\n",
    "print(\"predict:\", repr(\"\".join([idx2char[i] for i in sample_indice])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100)\n",
      "4.1744466\n"
     ]
    }
   ],
   "source": [
    "# from_logits=True：因为返回的是逻辑回归，所以设置TRUE\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(y_true=labels,\n",
    "                                                     y_pred=logits,\n",
    "                                                     from_logits=True,)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=loss)\n",
    "example_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(example_loss.shape)\n",
    "print(example_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - 535s 3s/step - loss: 2.5826\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 549s 3s/step - loss: 1.8956\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 549s 3s/step - loss: 1.6518\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 555s 3s/step - loss: 1.5154\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 656s 4s/step - loss: 1.4317\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 691s 4s/step - loss: 1.3748\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 646s 4s/step - loss: 1.3297\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 540s 3s/step - loss: 1.2906\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 541s 3s/step - loss: 1.2554\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 550s 3s/step - loss: 1.2216\n"
     ]
    }
   ],
   "source": [
    "# 设置检查点\n",
    "output_dir = \"./out_generation_checkpoints\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "checkpoint_prefix = os.path.join(output_dir, 'ckpt_{epoch}')\n",
    "# 保存权重信息\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    save_weights_only = True)\n",
    "\n",
    "epochs = 10\n",
    "history = model.fit(seq_dataset, epochs = epochs,\n",
    "                    callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[57 46  1 ... 43  1 45]\n",
      " [ 1 57 59 ... 53 44  1]\n",
      " [52  1 42 ... 39 56 43]\n",
      " ...\n",
      " [39 54  1 ... 58 53  1]\n",
      " [47 41 43 ... 61 53 56]\n",
      " [56 53 61 ... 21  1 58]], shape=(64, 100), dtype=int32) (64, 100)\n",
      "tf.Tensor(\n",
      "[[46  1 46 ...  1 45 39]\n",
      " [57 59 47 ... 44  1 46]\n",
      " [ 1 42 53 ... 56 43 61]\n",
      " ...\n",
      " [54  1 58 ... 53  1 45]\n",
      " [41 43  8 ... 53 56 42]\n",
      " [53 61 52 ...  1 58 46]], shape=(64, 100), dtype=int32) (64, 100)\n",
      "(64, 100, 65)\n",
      "input: 'sh him our city,\\nIn peril of precipitation\\nFrom off the rock Tarpeian never more\\nTo enter our Rome g'\n",
      "output: 'h him our city,\\nIn peril of precipitation\\nFrom off the rock Tarpeian never more\\nTo enter our Rome ga'\n",
      "predict: 'Gd&,mnMXicknmjZOv;.&3d&Fnm,3ss;ktuLOqC?DG&rAka,QN\\nYDGv;sN;:3ZXxEA.3R$pWZ\\nfzsk,poY3vXn.bnR,chFeMT,?LV'\n"
     ]
    }
   ],
   "source": [
    "# 使用训练好的模型\n",
    "for input_example_batch, target_example_batch in seq_dataset.take(1):\n",
    "    print(input_example_batch, input_example_batch.shape)\n",
    "    print(target_example_batch, target_example_batch.shape)\n",
    "    # 函数式调用\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape)\n",
    "    \n",
    "# 对比预测的数据\n",
    "print(\"input:\", repr(\"\".join([idx2char[i] for i in np.array(input_example_batch[0])])))\n",
    "print(\"output:\", repr(\"\".join([idx2char[i] for i in np.array(target_example_batch[0])])))\n",
    "print(\"predict:\", repr(\"\".join([idx2char[i] for i in sample_indice])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./out_generation_checkpoints\\\\ckpt_10'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 恢复最新的检查点\n",
    "# 为保持此次预测步骤简单，将批大小设定为 1。\n",
    "# 由于 RNN 状态从时间步传递到时间步的方式，模型建立好之后只接受固定的批大小。\n",
    "# 若要使用不同的 batch_size 来运行模型，我们需要重建模型并从检查点中恢复权重。\n",
    "model02 = build_model(vocab_size=vocab_size, embedding_dim=embedding_dim, batch_size=1, units=units)\n",
    "model02.load_weights(tf.train.latest_checkpoint(output_dir))\n",
    "model02.build(tf.TensorShape([1, None]))\n",
    "model02.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    input_eval = [char2idx[c] for c in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    num_generate = 1000\n",
    "    text_generate = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    for _ in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        #sample_indice = tf.random.categorical(logits=predictions[0], num_samples=1)\n",
    "        #sample_indice = tf.squeeze(input=sample_indices, axis=1)\n",
    "        text_generate.append(idx2char[predicted_id])\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return start_string + \"\".join(text_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: mina pine.\n",
      "\n",
      "Lord:\n",
      "Welcome, you shall held it a man dot.\n",
      "\n",
      "LEONTES:\n",
      "That it struke and after me; counterbine song!\n",
      "How now, he will go too cruel of mother.\n",
      "\n",
      "CAMILLO:\n",
      "My meerure find us mad: and I with\n",
      "feeling matter gulls again. Prince! \n",
      "ISABELLA:\n",
      "No, sir, your loviday, sir, interror inlk,\n",
      "All my fortune shave before by yecreat me the succesf;\n",
      "For Bariana A'll and Grumio as o'erwerr?\n",
      "\n",
      "GONZALO:\n",
      "'tis a good news, good fell with three honest present pears,' bossh,\n",
      "And that borne war speak in peace and stim at him\n",
      "And long i' the wash; and he used thyself.\n",
      "My gaim, he'le wardly, I can.\n",
      "\n",
      "FLORIZEL:\n",
      "Fear not, Kine with you, thy purpose by me,\n",
      "I hear me of my boots, bride alike:\n",
      "And, as you shall faint.\n",
      "\n",
      "PETRUCHIO:\n",
      "Where, the acept unjust can do not rish snumbling\n",
      "Should carry statutes him for pernipy; I\n",
      "say, sir; as he, easy holds,\n",
      "And bid you hear me over your husband!\n",
      "Here be your name; but then supposess is nent;\n",
      "And will recrieve twice, to my instructions upon you!\n",
      "\n",
      "APHORSON:\n",
      "Hord lords, b\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model02, \"All:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
