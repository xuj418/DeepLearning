{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./shakespeare.txt\", mode=\"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citi\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# 1.generate vocabulary\n",
    "vocabulary = sorted(set(text))\n",
    "print(len(vocabulary))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "# 2.build mapping\n",
    "char2idx = {value:key for key, value in enumerate(vocabulary)}\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "# 根据索引找文字\n",
    "idx2char = np.array(vocabulary)\n",
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47]\n",
      "First Citi\n"
     ]
    }
   ],
   "source": [
    "# 3.transfer from char to index\n",
    "text_as_index = [char2idx[c] for c in text]\n",
    "\n",
    "print(len(text_as_index))\n",
    "print(text_as_index[:10])\n",
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_tensor_slices: 数组 =》 dataset\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(18, shape=(), dtype=int32) F\n",
      "tf.Tensor(47, shape=(), dtype=int32) i\n"
     ]
    }
   ],
   "source": [
    "for i in char_dataset.take(2):\n",
    "    print(i, idx2char[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch: 分成batch大小的dataset\n",
    "# batch_size:表示要在单个批次中合并的此数据集的连续元素个数\n",
    "# drop_remainder：表示在少于batch_size元素的情况下是否应删除最后一批\n",
    "\n",
    "seq_length = 100\n",
    "seq_dataset = char_dataset.batch(batch_size=seq_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11043"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1115394 / 101 = 11043\n",
    "len(seq_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59  1], shape=(101,), dtype=int32)\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "tf.Tensor(\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1 49], shape=(101,), dtype=int32)\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
     ]
    }
   ],
   "source": [
    "# 查看数据集元素\n",
    "for seq in seq_dataset.take(2):\n",
    "    print(seq)\n",
    "    # idx => char\n",
    "    print(repr(\"\".join([idx2char[index] for index in seq])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.预测文本 \n",
    "# data: abcd, return: abc,bcd\n",
    "def split_input_target(data):\n",
    "    return data[0:-1], data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59], shape=(100,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1\n",
      " 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
      " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6\n",
      "  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0\n",
      " 37 53 59  1], shape=(100,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1], shape=(100,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1 58\n",
      " 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0 13\n",
      " 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8  0\n",
      "  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1 63\n",
      " 53 59  1 49], shape=(100,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "seq_dataset = seq_dataset.map(split_input_target)\n",
    "\n",
    "# inputdata, outputdata: shape=100\n",
    "for inputdata, outputdata in seq_dataset.take(2):\n",
    "    print(inputdata)\n",
    "    print(outputdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 1000\n",
    "\n",
    "seq_dataset = seq_dataset.shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder=True)\n",
    "\n",
    "# 11043/64 = 172\n",
    "print(len(seq_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, batch_size, units):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(\n",
    "            input_dim = vocab_size, \n",
    "            output_dim = embedding_dim, \n",
    "            # [batch_size, None]:元素个数确定， 其他未定\n",
    "            batch_input_shape = [batch_size, None]),\n",
    "        keras.layers.GRU(\n",
    "            # units：输出空间的维度\n",
    "            # stateful：默认 False，如果为 True，则批次中索引 i 处的每个样品的最后状态将用作下一批次中索引 i 样品的初始状态。\n",
    "            # recurrent_initializer\n",
    "            # return_sequences：是返回输出序列中的最后一个输出，还是全部序列\n",
    "            units=units, \n",
    "            stateful = True,\n",
    "            recurrent_initializer = 'glorot_uniform',\n",
    "            return_sequences=True),\n",
    "        keras.layers.Dense(vocab_size),\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "batch_size = 64\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, batch_size, units)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[53 59 56 ... 59 57 10]\n",
      " [35 46 43 ... 58  1 47]\n",
      " [53 40 50 ...  1 42 53]\n",
      " ...\n",
      " [47 43 57 ...  6  0 42]\n",
      " [ 1 44 47 ...  1 44 39]\n",
      " [61  1 58 ...  1 39  1]], shape=(64, 100), dtype=int32) (64, 100)\n",
      "tf.Tensor(\n",
      "[[59 56  1 ... 57 10  0]\n",
      " [46 43 52 ...  1 47 57]\n",
      " [40 50 43 ... 42 53  1]\n",
      " ...\n",
      " [43 57  8 ...  0 42 47]\n",
      " [44 47 56 ... 44 39 57]\n",
      " [ 1 58 46 ... 39  1 51]], shape=(64, 100), dtype=int32) (64, 100)\n",
      "(64, 100, 65)\n"
     ]
    }
   ],
   "source": [
    "# 对于每个字符，模型会查找嵌入，把嵌入当作输入运行一个时间步，并用密集层生成逻辑回归 （logits），预测下一个字符的对数可能性。\n",
    "# 预测文本\n",
    "for input_example_batch, target_example_batch in seq_dataset.take(1):\n",
    "    print(input_example_batch, input_example_batch.shape)\n",
    "    print(target_example_batch, target_example_batch.shape)\n",
    "    # 函数式调用\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[17]\n",
      " [59]\n",
      " [30]\n",
      " [ 6]\n",
      " [56]\n",
      " [47]\n",
      " [20]\n",
      " [ 9]\n",
      " [48]\n",
      " [37]\n",
      " [63]\n",
      " [10]\n",
      " [49]\n",
      " [48]\n",
      " [10]\n",
      " [32]\n",
      " [32]\n",
      " [38]\n",
      " [48]\n",
      " [ 9]\n",
      " [14]\n",
      " [28]\n",
      " [56]\n",
      " [38]\n",
      " [ 1]\n",
      " [41]\n",
      " [43]\n",
      " [31]\n",
      " [49]\n",
      " [49]\n",
      " [30]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [40]\n",
      " [37]\n",
      " [47]\n",
      " [13]\n",
      " [18]\n",
      " [ 2]\n",
      " [27]\n",
      " [58]\n",
      " [57]\n",
      " [ 6]\n",
      " [25]\n",
      " [43]\n",
      " [53]\n",
      " [22]\n",
      " [36]\n",
      " [28]\n",
      " [21]\n",
      " [ 8]\n",
      " [52]\n",
      " [25]\n",
      " [64]\n",
      " [43]\n",
      " [ 7]\n",
      " [27]\n",
      " [34]\n",
      " [60]\n",
      " [26]\n",
      " [14]\n",
      " [63]\n",
      " [34]\n",
      " [50]\n",
      " [27]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [49]\n",
      " [20]\n",
      " [24]\n",
      " [53]\n",
      " [ 7]\n",
      " [41]\n",
      " [42]\n",
      " [35]\n",
      " [ 0]\n",
      " [24]\n",
      " [46]\n",
      " [49]\n",
      " [62]\n",
      " [34]\n",
      " [19]\n",
      " [15]\n",
      " [31]\n",
      " [60]\n",
      " [61]\n",
      " [41]\n",
      " [ 7]\n",
      " [35]\n",
      " [33]\n",
      " [30]\n",
      " [ 2]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [31]\n",
      " [58]\n",
      " [52]\n",
      " [42]\n",
      " [59]\n",
      " [10]], shape=(100, 1), dtype=int64)\n",
      "tf.Tensor(\n",
      "[17 59 30  6 56 47 20  9 48 37 63 10 49 48 10 32 32 38 48  9 14 28 56 38\n",
      "  1 41 43 31 49 49 30  5  1 40 37 47 13 18  2 27 58 57  6 25 43 53 22 36\n",
      " 28 21  8 52 25 64 43  7 27 34 60 26 14 63 34 50 27  1  2 49 20 24 53  7\n",
      " 41 42 35  0 24 46 49 62 34 19 15 31 60 61 41  7 35 33 30  2  7  5 31 58\n",
      " 52 42 59 10], shape=(100,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# tf.random.categorical：从一个分类分布中抽取样本\n",
    "# logits: 形状为 [batch_size, num_classes]的张量\n",
    "# num_samples：从每一行切片中抽取的独立样本的数量\n",
    "# [batch_size * num_classes] => [batch_size * num_samples] , [100, 65] => [100, 1]\n",
    "sample_indices = tf.random.categorical(logits=example_batch_predictions[0], num_samples=1)\n",
    "print(sample_indices)\n",
    "\n",
    "# 从input中删除一个纬度\n",
    "sample_indice = tf.squeeze(input=sample_indices, axis=1)\n",
    "print(sample_indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 'our shields before your hearts, and fight\\nWith hearts more proof than shields. Advance,\\nbrave Titus:'\n",
      "output: 'ur shields before your hearts, and fight\\nWith hearts more proof than shields. Advance,\\nbrave Titus:\\n'\n",
      "predict: \"EuR,riH3jYy:kj:TTZj3BPrZ ceSkkR' bYiAF!Ots,MeoJXPI.nMze-OVvNByVlO !kHLo-cdW\\nLhkxVGCSvwc-WUR!-'Stndu:\"\n"
     ]
    }
   ],
   "source": [
    "# 对比预测的数据\n",
    "print(\"input:\", repr(\"\".join([idx2char[i] for i in np.array(input_example_batch[0])])))\n",
    "print(\"output:\", repr(\"\".join([idx2char[i] for i in np.array(target_example_batch[0])])))\n",
    "print(\"predict:\", repr(\"\".join([idx2char[i] for i in sample_indice])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100)\n",
      "4.1721706\n"
     ]
    }
   ],
   "source": [
    "# from_logits=True：因为返回的是逻辑回归，所以设置TRUE\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(y_true=labels,\n",
    "                                                     y_pred=logits,\n",
    "                                                     from_logits=True,)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=loss)\n",
    "example_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(example_loss.shape)\n",
    "print(example_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - 335s 2s/step - loss: 2.6830\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 346s 2s/step - loss: 1.9767\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 340s 2s/step - loss: 1.7230\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 343s 2s/step - loss: 1.5653\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 342s 2s/step - loss: 1.4685\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 340s 2s/step - loss: 1.4045\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 339s 2s/step - loss: 1.3562\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 341s 2s/step - loss: 1.3167\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 939s 5s/step - loss: 1.2808\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 340s 2s/step - loss: 1.2471\n"
     ]
    }
   ],
   "source": [
    "# 设置检查点\n",
    "output_dir = \"./out_generation_checkpoints\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "checkpoint_prefix = os.path.join(output_dir, 'ckpt_{epoch}')\n",
    "# 保存权重信息\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    save_weights_only = True)\n",
    "\n",
    "epochs = 10\n",
    "history = model.fit(seq_dataset, epochs = epochs,\n",
    "                    callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[53  1 46 ... 41 53 52]\n",
      " [52 53 61 ... 57  1 49]\n",
      " [43  1 56 ... 21  1 40]\n",
      " ...\n",
      " [ 2  0  0 ... 41 49  6]\n",
      " [43  1 39 ... 53  1 49]\n",
      " [ 1 58 46 ... 57  1 51]], shape=(64, 100), dtype=int32) (64, 100)\n",
      "tf.Tensor(\n",
      "[[ 1 46 43 ... 53 52 42]\n",
      " [53 61  1 ...  1 49 47]\n",
      " [ 1 56 53 ...  1 40 39]\n",
      " ...\n",
      " [ 0  0 34 ... 49  6  1]\n",
      " [ 1 39 56 ...  1 49 43]\n",
      " [58 46 43 ...  1 51 59]], shape=(64, 100), dtype=int32) (64, 100)\n",
      "(64, 100, 65)\n",
      "input: \"o help Cominius.\\n\\nLARTIUS:\\nWorthy sir, thou bleed'st;\\nThy exercise hath been too violent for\\nA secon\"\n",
      "output: \" help Cominius.\\n\\nLARTIUS:\\nWorthy sir, thou bleed'st;\\nThy exercise hath been too violent for\\nA second\"\n",
      "predict: \"EuR,riH3jYy:kj:TTZj3BPrZ ceSkkR' bYiAF!Ots,MeoJXPI.nMze-OVvNByVlO !kHLo-cdW\\nLhkxVGCSvwc-WUR!-'Stndu:\"\n"
     ]
    }
   ],
   "source": [
    "# 使用训练好的模型\n",
    "for input_example_batch, target_example_batch in seq_dataset.take(1):\n",
    "    print(input_example_batch, input_example_batch.shape)\n",
    "    print(target_example_batch, target_example_batch.shape)\n",
    "    # 函数式调用\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape)\n",
    "    \n",
    "# 对比预测的数据\n",
    "print(\"input:\", repr(\"\".join([idx2char[i] for i in np.array(input_example_batch[0])])))\n",
    "print(\"output:\", repr(\"\".join([idx2char[i] for i in np.array(target_example_batch[0])])))\n",
    "print(\"predict:\", repr(\"\".join([idx2char[i] for i in sample_indice])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./out_generation_checkpoints\\\\ckpt_10'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 恢复最新的检查点\n",
    "# 为保持此次预测步骤简单，将批大小设定为 1。\n",
    "# 由于 RNN 状态从时间步传递到时间步的方式，模型建立好之后只接受固定的批大小。\n",
    "# 若要使用不同的 batch_size 来运行模型，我们需要重建模型并从检查点中恢复权重。\n",
    "model02 = build_model(vocab_size=vocab_size, embedding_dim=embedding_dim, batch_size=1, units=units)\n",
    "model02.load_weights(tf.train.latest_checkpoint(output_dir))\n",
    "model02.build(tf.TensorShape([1, None]))\n",
    "model02.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    input_eval = [char2idx[c] for c in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    num_generate = 1000\n",
    "    text_generate = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    for _ in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        #sample_indice = tf.random.categorical(logits=predictions[0], num_samples=1)\n",
    "        #sample_indice = tf.squeeze(input=sample_indices, axis=1)\n",
    "        text_generate.append(idx2char[predicted_id])\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return start_string + \"\".join(text_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: answer thies? E thank you, sir, which; though he would hear\n",
      "The subjesher than ourserves, abtake or sheek;\n",
      "And, fie, fie, father,.\n",
      "\n",
      "BAPTISTA:\n",
      "\n",
      "Cantil. I pray.\n",
      "\n",
      "FROTH:\n",
      "Why, there nemes, I know on 't. And how, no!\n",
      "God full of request, I'll give with thee: Frey? or I'll bear myselves. Was it shall heard thee,\n",
      "Than that he would brings them virtuous countrigment,\n",
      "Ortentle, make thee might rough all, I said\n",
      "against the impatch to met on youth you say, sir.\n",
      "\n",
      "MIRANDA:\n",
      "City, wretch: marry, sir, Hastings are,\n",
      "Nidecyon me, gentle inquest of -indeeterance\n",
      "In her trust in hability, shr dreams there; you hear,\n",
      "If you that duintly not in powerful raze with some fire, spural!\n",
      "\n",
      "WARWICK:\n",
      "O, sir?\n",
      "Happosts how? I find you, sir, I would possess to\n",
      "Richard is it tongues that Lewis ask for 's.\n",
      "\n",
      "SEBASTIAN:\n",
      "A valling swenter on Cates.\n",
      "\n",
      "ISABELLA:\n",
      "O trephem Helpefore.\n",
      "\n",
      "GREMIO:\n",
      "Ay, marry, my leave, I fear me.\n",
      "\n",
      "MIRANDA:\n",
      "Lenceare my father; being order:\n",
      "I'll make thee so shall my youngest house-tells;\n",
      "Because sup\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model02, \"All:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
