{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = housing.data, housing.target\n",
    "X_train_all, X_test, y_train_all, y_test = train_test_split(X, y, random_state=7)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_all, y_train_all, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_scale = StandardScaler()\n",
    "\n",
    "x_train_scale = stand_scale.fit_transform(X_train)\n",
    "x_valid_scale = stand_scale.transform(X_valid)\n",
    "x_test_scale = stand_scale.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (5160,)\n",
      "(5160, 8) (3870,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_scale.shape, y_train.shape)\n",
    "print(x_valid_scale.shape, y_test.shape)\n",
    "print(x_test_scale.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11610"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"generate_csv\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# output_dir:处理文件路径， data：出力数据源\n",
    "# header：头文件信息， name_prefix：处理文件命名用\n",
    "# n_parts：将data拆分成多少份\n",
    "def save_to_csv(output_dir, data, header, name_prefix, n_parts):\n",
    "    \n",
    "    # 定义出力文件格式\n",
    "    path_format = os.path.join(output_dir, \"{}_{:02d}.csv\")\n",
    "    file_names = []\n",
    "    \n",
    "    # 将data中的样本数据拆分成n_parts份\n",
    "    # file_idx:00, 01, 02, 03.....\n",
    "    # row_indices = [0, 1, ...19],[20, 21,...]\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(len(data)), n_parts)):\n",
    "        # 生成文件名\n",
    "        file_name = path_format.format(name_prefix, file_idx)\n",
    "        file_names.append(file_name)\n",
    "        \n",
    "        # 写文件\n",
    "        with open(file_name, mode=\"w\") as f:\n",
    "            # 生成头文件\n",
    "            if (header is not None):\n",
    "                f.write(header + \"\\n\")\n",
    "            \n",
    "            # 生成数据\n",
    "            for row_indice in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_indice]]))\n",
    "                f.write(\"\\n\")\n",
    "    \n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并数据\n",
    "train_data = np.c_[x_train_scale, y_train]\n",
    "valid_data = np.c_[x_valid_scale, y_valid]\n",
    "test_data = np.c_[x_test_scale, y_test]\n",
    "\n",
    "housing_header = housing.feature_names\n",
    "housing_header.append(\"MiddlePrice\")\n",
    "header_str = \",\".join(housing_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MiddlePrice'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11610/20=581, 一个csv文件581条数据\n",
    "train_filenames = save_to_csv(output_dir, train_data, header_str, \"train\", n_parts=20)\n",
    "valid_filenames = save_to_csv(output_dir, valid_data, header_str, \"valid\", n_parts=10)\n",
    "test_filenames = save_to_csv(output_dir, test_data, header_str, \"test\", n_parts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_dataset = tf.data.Dataset.list_files(train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'generate_csv\\\\train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv\\\\train_14.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for filename in filename_dataset:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = filename_dataset.interleave(\n",
    "    lambda filename:tf.data.TextLineDataset(filename).skip(1),\n",
    "    cycle_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'0.801544314532886,0.27216142415910205,-0.11624392696666119,-0.2023115137272354,-0.5430515742518128,-0.021039615516440048,-0.5897620622908205,-0.08241845654707416,3.226'\n",
      "b'-1.0591781535672364,1.393564736946074,-0.026331968874673636,-0.11006759528831847,-0.6138198966579805,-0.09695934953589447,0.3247131133362288,-0.037477245413977976,0.672'\n",
      "b'-1.0775077698160966,-0.44874070548966555,-0.5680568205591913,-0.14269262164909954,-0.09666677138213985,0.12326468238687088,-0.3144863716683942,-0.4818958888413162,0.978'\n",
      "b'-0.32652634129448693,0.43236189741438374,-0.09345459539684739,-0.08402991822890092,0.8460035745154013,-0.0266316482653991,-0.5617679242614233,0.1422875991184281,2.431'\n",
      "b'0.4853051504718848,-0.8492418886278699,-0.06530126513877861,-0.023379656040017353,1.4974350551260218,-0.07790657783453239,-0.9023632702857819,0.7814514907892068,2.956'\n",
      "b'0.6363646332204844,-1.0895425985107923,0.09260902815633619,-0.20538124656801682,1.2025670451003232,-0.03630122549633783,-0.6784101660505877,0.182235342347858,2.429'\n",
      "b'0.15782311132800697,0.43236189741438374,0.3379948076652917,-0.015880306122244434,-0.3733890577139493,-0.05305245634489608,0.8006134598360177,-1.2359095422966828,3.169'\n",
      "b'-1.1157655153587753,0.9930635538078697,-0.33419201318312125,-0.0653521844775239,-0.3289320346639209,0.04343065774347637,-0.12785878480573185,0.30707203993980686,0.524'\n",
      "b'0.04326300977263167,-1.0895425985107923,-0.38878716774583305,-0.10789864528874438,-0.6818663605100649,-0.0723871014747467,-0.8883662012710817,0.8213992340186296,1.426'\n",
      "b'-0.09719300311107498,-1.249743071766074,0.36232962250170797,0.026906080250728295,1.033811814747154,0.045881586971778555,1.3418334617377423,-1.6353869745909178,1.832'\n",
      "b'2.51504373119231,1.0731637904355105,0.5574401201546321,-0.17273513019187772,-0.612912610473286,-0.01909156503651574,-0.5710993036045546,-0.027490309606616956,5.00001'\n",
      "b'-0.8219588176953616,1.874166156711919,0.18212349433218608,-0.03170019246279883,-0.6011178900722581,-0.14337494105109344,1.0852205298015787,-0.8613994495208361,1.054'\n",
      "b'0.8115083791797953,-0.04823952235146133,0.5187339067174729,-0.029386394873127775,-0.034064024638222286,-0.05081594842905086,-0.7157356834231196,0.9162751241885168,2.147'\n",
      "b'0.04971034572063198,-0.8492418886278699,-0.06214699417830008,0.17878747064657746,-0.8025354230744277,0.0005066066922077538,0.6466457006743215,-1.1060793768010604,2.286'\n",
      "b'0.6303435674178064,1.874166156711919,-0.06713214279531016,-0.12543366804152128,-0.19737553788322462,-0.022722631725889016,-0.692407235065288,0.7265233438487496,2.419'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(15):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=2>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=3>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=4>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=5>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_records = \"1,2,3,4,5\"\n",
    "record_defaults = [tf.constant(0)] * 5\n",
    "\n",
    "# tf.io.decode_csv => tf.Tensor\n",
    "# record_defaults.size = sample_records.size\n",
    "tf.io.decode_csv(\n",
    "    sample_records,\n",
    "    record_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=int32, numpy=1>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n",
       " <tf.Tensor: shape=(), dtype=string, numpy=b'3'>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=4.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_records = \"1,2,3,4,5\"\n",
    "# int,float,str,float,float\n",
    "record_defaults = [\n",
    "    tf.constant(0),\n",
    "    tf.constant(1.0),\n",
    "    \"str\",\n",
    "    np.nan,\n",
    "    1.0\n",
    "]\n",
    "\n",
    "tf.io.decode_csv(\n",
    "    sample_records,\n",
    "    record_defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect 5 fields but have 4 in record 0 [Op:DecodeCSV]\n"
     ]
    }
   ],
   "source": [
    "# 数量必须一致\n",
    "try:\n",
    "    tf.io.decode_csv(\",,,\", record_defaults)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析单条记录\n",
    "def parse_to_csv(records, n_records=9):\n",
    "    dfs = [np.nan] * n_records\n",
    "    dataset = tf.io.decode_csv(records, dfs)\n",
    "    x = tf.stack(dataset[0:-1])  # tf.stack=>多个tensor合并为一个\n",
    "    y = dataset[-1]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([-0.097193  , -1.2497431 ,  0.36232963,  0.02690608,  1.0338118 ,\n",
       "         0.04588159,  1.3418335 , -1.635387  ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.832>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_to_csv(b'-0.09719300311107498,-1.249743071766074,0.36232962250170797,0.026906080250728295,1.033811814747154,0.045881586971778555,1.3418334617377423,-1.6353869745909178,1.832',\n",
    "            9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.filename => dataset\n",
    "# 2.read_file => dataset => datasets => merge\n",
    "# 3.parse csv\n",
    "\n",
    "def csv_reader_dataset(filenames, batch_size):\n",
    "    # 读入文件名\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    print(\"1:\")\n",
    "    print(dataset)\n",
    "    \n",
    "    # 读入文件中的数据\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename:tf.data.TextLineDataset(filename).skip(1),\n",
    "        cycle_length = 5)\n",
    "    print(\"2:\")\n",
    "    print(dataset)\n",
    "    \n",
    "    # 对文件中的数据转换\n",
    "    dataset = dataset.map(parse_to_csv)\n",
    "    print(\"3:\")\n",
    "    print(dataset)\n",
    "    \n",
    "    # 生成batch_size的文件\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    print(\"4:\")\n",
    "    print(dataset)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.Dataset.list_files(train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_csv\\\\train_00.csv',\n",
      " 'generate_csv\\\\train_01.csv',\n",
      " 'generate_csv\\\\train_02.csv',\n",
      " 'generate_csv\\\\train_03.csv',\n",
      " 'generate_csv\\\\train_04.csv',\n",
      " 'generate_csv\\\\train_05.csv',\n",
      " 'generate_csv\\\\train_06.csv',\n",
      " 'generate_csv\\\\train_07.csv',\n",
      " 'generate_csv\\\\train_08.csv',\n",
      " 'generate_csv\\\\train_09.csv',\n",
      " 'generate_csv\\\\train_10.csv',\n",
      " 'generate_csv\\\\train_11.csv',\n",
      " 'generate_csv\\\\train_12.csv',\n",
      " 'generate_csv\\\\train_13.csv',\n",
      " 'generate_csv\\\\train_14.csv',\n",
      " 'generate_csv\\\\train_15.csv',\n",
      " 'generate_csv\\\\train_16.csv',\n",
      " 'generate_csv\\\\train_17.csv',\n",
      " 'generate_csv\\\\train_18.csv',\n",
      " 'generate_csv\\\\train_19.csv']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:\n",
      "<ShuffleDataset shapes: (), types: tf.string>\n",
      "2:\n",
      "<InterleaveDataset shapes: (), types: tf.string>\n",
      "3:\n",
      "<MapDataset shapes: ((8,), ()), types: (tf.float32, tf.float32)>\n",
      "4:\n",
      "<BatchDataset shapes: ((None, 8), (None,)), types: (tf.float32, tf.float32)>\n",
      "<BatchDataset shapes: ((None, 8), (None,)), types: (tf.float32, tf.float32)>\n",
      "tf.Tensor(\n",
      "[[ 0.81150836 -0.04823952  0.5187339  -0.0293864  -0.03406402 -0.05081595\n",
      "  -0.7157357   0.91627514]\n",
      " [-1.1157656   0.99306357 -0.334192   -0.06535219 -0.32893205  0.04343066\n",
      "  -0.12785879  0.30707204]\n",
      " [ 0.63034356  1.8741661  -0.06713215 -0.12543367 -0.19737554 -0.02272263\n",
      "  -0.69240725  0.72652334]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor([2.147 0.524 2.419], shape=(3,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.48530516 -0.8492419  -0.06530126 -0.02337966  1.4974351  -0.07790658\n",
      "  -0.90236324  0.78145146]\n",
      " [-1.119975   -1.3298433   0.14190045  0.4658137  -0.10301778 -0.10744184\n",
      "  -0.7950524   1.5304717 ]\n",
      " [-0.69061434 -0.12833975  7.020181    5.6242876  -0.2663293  -0.0366208\n",
      "  -0.64575034  1.2058963 ]], shape=(3, 8), dtype=float32)\n",
      "tf.Tensor([2.956 0.66  1.352], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "train_set = csv_reader_dataset(train_filenames, batch_size=3)\n",
    "print(train_set)\n",
    "\n",
    "# 取两组数据，一组有三个\n",
    "for x_train, y_train in train_set.take(2):\n",
    "    print(x_train)\n",
    "    print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:\n",
      "<ShuffleDataset shapes: (), types: tf.string>\n",
      "2:\n",
      "<InterleaveDataset shapes: (), types: tf.string>\n",
      "3:\n",
      "<MapDataset shapes: ((8,), ()), types: (tf.float32, tf.float32)>\n",
      "4:\n",
      "<BatchDataset shapes: ((None, 8), (None,)), types: (tf.float32, tf.float32)>\n",
      "1:\n",
      "<ShuffleDataset shapes: (), types: tf.string>\n",
      "2:\n",
      "<InterleaveDataset shapes: (), types: tf.string>\n",
      "3:\n",
      "<MapDataset shapes: ((8,), ()), types: (tf.float32, tf.float32)>\n",
      "4:\n",
      "<BatchDataset shapes: ((None, 8), (None,)), types: (tf.float32, tf.float32)>\n",
      "1:\n",
      "<ShuffleDataset shapes: (), types: tf.string>\n",
      "2:\n",
      "<InterleaveDataset shapes: (), types: tf.string>\n",
      "3:\n",
      "<MapDataset shapes: ((8,), ()), types: (tf.float32, tf.float32)>\n",
      "4:\n",
      "<BatchDataset shapes: ((None, 8), (None,)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "train_set = csv_reader_dataset(train_filenames, batch_size=32)\n",
    "valid_set = csv_reader_dataset(valid_filenames, batch_size=32)\n",
    "test_set = csv_reader_dataset(test_filenames, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 1.8096 - val_loss: 1.1005\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.8230 - val_loss: 0.8137\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 0.6917 - val_loss: 0.7232\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 2s 7ms/step - loss: 0.6524 - val_loss: 0.6736TA: 0s - loss:\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 3s 8ms/step - loss: 0.6026 - val_loss: 0.6383\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 0.5645 - val_loss: 0.6128\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.5509 - val_loss: 0.5911\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 4s 10ms/step - loss: 0.5313 - val_loss: 0.5711\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.5089 - val_loss: 0.5578\n",
      "Epoch 10/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.5124 - val_loss: 0.5446\n",
      "Epoch 11/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.4997 - val_loss: 0.5321\n",
      "Epoch 12/100\n",
      "348/348 [==============================] - 3s 10ms/step - loss: 0.4792 - val_loss: 0.5240\n",
      "Epoch 13/100\n",
      "348/348 [==============================] - 3s 10ms/step - loss: 0.4879 - val_loss: 0.5155\n",
      "Epoch 14/100\n",
      "348/348 [==============================] - 2s 7ms/step - loss: 0.4635 - val_loss: 0.5104\n",
      "Epoch 15/100\n",
      "348/348 [==============================] - 4s 11ms/step - loss: 0.4795 - val_loss: 0.5038\n",
      "Epoch 16/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 0.4567 - val_loss: 0.4980\n",
      "Epoch 17/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.4682 - val_loss: 0.4941\n",
      "Epoch 18/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.4518 - val_loss: 0.4875\n",
      "Epoch 19/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 0.4524 - val_loss: 0.4848\n",
      "Epoch 20/100\n",
      "348/348 [==============================] - 4s 12ms/step - loss: 0.4415 - val_loss: 0.4802\n",
      "Epoch 21/100\n",
      "348/348 [==============================] - 3s 10ms/step - loss: 0.4485 - val_loss: 0.4768\n",
      "Epoch 22/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.4396 - val_loss: 0.4746\n",
      "Epoch 23/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.4457 - val_loss: 0.4721\n",
      "Epoch 24/100\n",
      "348/348 [==============================] - 4s 10ms/step - loss: 0.4356 - val_loss: 0.4691- loss: 0.436\n",
      "Epoch 25/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.4382 - val_loss: 0.4671\n",
      "Epoch 26/100\n",
      "348/348 [==============================] - 2s 7ms/step - loss: 0.4310 - val_loss: 0.4640\n",
      "Epoch 27/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.4332 - val_loss: 0.4614\n",
      "Epoch 28/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.4242 - val_loss: 0.4590\n",
      "Epoch 29/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.4345 - val_loss: 0.4574\n",
      "Epoch 30/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.4208 - val_loss: 0.4545\n",
      "Epoch 31/100\n",
      "348/348 [==============================] - 2s 7ms/step - loss: 0.4315 - val_loss: 0.4540\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=[8]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.SGD(0.001))\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=5, min_delta=1e-2)]\n",
    "\n",
    "batch_size= 32\n",
    "history = model.fit(train_set.repeat(),   # 加上repeat\n",
    "                    validation_data = valid_set,\n",
    "                    steps_per_epoch = 11160 // batch_size,\n",
    "                    validation_steps = 3870 // batch_size,\n",
    "                    epochs = 100,\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
