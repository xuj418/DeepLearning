{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_csv\\\\train_00.csv',\n",
      " 'generate_csv\\\\train_01.csv',\n",
      " 'generate_csv\\\\train_02.csv',\n",
      " 'generate_csv\\\\train_03.csv',\n",
      " 'generate_csv\\\\train_04.csv',\n",
      " 'generate_csv\\\\train_05.csv',\n",
      " 'generate_csv\\\\train_06.csv',\n",
      " 'generate_csv\\\\train_07.csv',\n",
      " 'generate_csv\\\\train_08.csv',\n",
      " 'generate_csv\\\\train_09.csv',\n",
      " 'generate_csv\\\\train_10.csv',\n",
      " 'generate_csv\\\\train_11.csv',\n",
      " 'generate_csv\\\\train_12.csv',\n",
      " 'generate_csv\\\\train_13.csv',\n",
      " 'generate_csv\\\\train_14.csv',\n",
      " 'generate_csv\\\\train_15.csv',\n",
      " 'generate_csv\\\\train_16.csv',\n",
      " 'generate_csv\\\\train_17.csv',\n",
      " 'generate_csv\\\\train_18.csv',\n",
      " 'generate_csv\\\\train_19.csv']\n",
      "['generate_csv\\\\valid_00.csv',\n",
      " 'generate_csv\\\\valid_01.csv',\n",
      " 'generate_csv\\\\valid_02.csv',\n",
      " 'generate_csv\\\\valid_03.csv',\n",
      " 'generate_csv\\\\valid_04.csv',\n",
      " 'generate_csv\\\\valid_05.csv',\n",
      " 'generate_csv\\\\valid_06.csv',\n",
      " 'generate_csv\\\\valid_07.csv',\n",
      " 'generate_csv\\\\valid_08.csv',\n",
      " 'generate_csv\\\\valid_09.csv']\n",
      "['generate_csv\\\\test_00.csv',\n",
      " 'generate_csv\\\\test_01.csv',\n",
      " 'generate_csv\\\\test_02.csv',\n",
      " 'generate_csv\\\\test_03.csv',\n",
      " 'generate_csv\\\\test_04.csv',\n",
      " 'generate_csv\\\\test_05.csv',\n",
      " 'generate_csv\\\\test_06.csv',\n",
      " 'generate_csv\\\\test_07.csv',\n",
      " 'generate_csv\\\\test_08.csv',\n",
      " 'generate_csv\\\\test_09.csv']\n"
     ]
    }
   ],
   "source": [
    "# 1.获取文件名\n",
    "def get_filenames_by_prefix(source_file, prefix):\n",
    "    all_files = os.listdir(source_file)\n",
    "    result = []\n",
    "    for file in all_files:\n",
    "        if file.startswith(prefix):\n",
    "            result.append(os.path.join(source_file, file))\n",
    "    return result\n",
    "            \n",
    "source_file = \"generate_csv\"\n",
    "train_name = get_filenames_by_prefix(source_file, \"train\")\n",
    "valid_name = get_filenames_by_prefix(source_file, \"valid\")\n",
    "test_name = get_filenames_by_prefix(source_file, \"test\")\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(train_name)\n",
    "pprint.pprint(valid_name)\n",
    "pprint.pprint(test_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.解析csv数据\n",
    "def parse_to_csv(records, colums=9):\n",
    "    default_records = [tf.constant(1.0)] * colums\n",
    "    data = tf.io.decode_csv(records, default_records)\n",
    "    x = tf.stack(data[0:-1])\n",
    "    y = tf.stack(data[-1])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.csv转dataset\n",
    "def csv_to_dataset(filenames, batchsize):\n",
    "    # a.读入文件名\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    \n",
    "    # b.读入文件中的数据  TextLineDataset\n",
    "    dataset = dataset.interleave(lambda file: tf.data.TextLineDataset(file).skip(1), cycle_length = 5)\n",
    "    \n",
    "    # c.对文件中的数据转换\n",
    "    dataset = dataset.map(parse_to_csv)\n",
    "    \n",
    "    # d.生成batch_size的文件\n",
    "    dataset = dataset.batch(batchsize)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_to_dataset(train_name, 32)\n",
    "valid_set = csv_to_dataset(valid_name, 32)\n",
    "test_set = csv_to_dataset(test_name, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.序列化\n",
    "def serialize_example(x, y):\n",
    "    # a.生成tf.train.Feature(bytes_list, float_list, int64_list)\n",
    "    input_features = tf.train.FloatList(value = x)\n",
    "    label = tf.train.FloatList(value = y)\n",
    "    \n",
    "    # b.生成tf.train.Features(feature = {\"key\": XXXX})\n",
    "    features = tf.train.Features(feature={\n",
    "        \"input_features\": tf.train.Feature(float_list = input_features),\n",
    "        \"label\": tf.train.Feature(float_list = label)\n",
    "    })\n",
    "    \n",
    "    # c.生成tf.train.Example\n",
    "    example = tf.train.Example(features = features)\n",
    "    \n",
    "    # d.example的序列化\n",
    "    serialize_example = example.SerializeToString()\n",
    "    \n",
    "    return serialize_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.csv转tfrecord，并写到文件中\n",
    "# nshapes：存多少个文件，steps_per_shard：遍历多少步\n",
    "def csv_dataset_to_tfrecords(base_filename, dataset, nshapes, steps_per_shard, compression_type=None):\n",
    "    options = tf.io.TFRecordOptions(compression_type=compression_type)\n",
    "    result = []\n",
    "    for shape_id in range(nshapes):\n",
    "        \n",
    "        fullfile_name = \"{}_{:05d}-of-{:05d}\".format(base_filename, shape_id, nshapes)\n",
    "        \n",
    "        with tf.io.TFRecordWriter(path = fullfile_name, options=options) as writer:\n",
    "            \n",
    "            for x_batch, y_batch in dataset.take(steps_per_shard):\n",
    "                for x, y in zip(x_batch, y_batch):\n",
    "                    writer.write(serialize_example(x.numpy(), [y.numpy()]))\n",
    "                    \n",
    "        result.append(fullfile_name)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 6.生成tfrecords文件\n",
    "base_filename = \"generate_tfrecord\"\n",
    "batch_size = 32\n",
    "nshapes = 20\n",
    "\n",
    "train_steps_per_shard = 11610 // batch_size // nshapes\n",
    "valid_steps_per_shard = 3880 // batch_size // nshapes\n",
    "test_steps_per_shard = 5170 // batch_size // nshapes\n",
    "\n",
    "if not os.path.exists(base_filename):\n",
    "    os.makedirs(base_filename)\n",
    "    \n",
    "train_base_filename = os.path.join(base_filename, \"train\")\n",
    "valid_base_filename = os.path.join(base_filename, \"valid\")\n",
    "test_base_filename = os.path.join(base_filename, \"test\")\n",
    "\n",
    "train_tfrecord_filenames = csv_dataset_to_tfrecords(train_base_filename, train_set, nshapes, train_steps_per_shard)\n",
    "valid_tfrecord_filenames = csv_dataset_to_tfrecords(valid_base_filename, valid_set, nshapes, valid_steps_per_shard)\n",
    "test_tfrecord_filenames = csv_dataset_to_tfrecords(test_base_filename, test_set, nshapes, test_steps_per_shard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['generate_tfrecord\\\\train_00000-of-00020',\n",
      " 'generate_tfrecord\\\\train_00001-of-00020',\n",
      " 'generate_tfrecord\\\\train_00002-of-00020',\n",
      " 'generate_tfrecord\\\\train_00003-of-00020',\n",
      " 'generate_tfrecord\\\\train_00004-of-00020',\n",
      " 'generate_tfrecord\\\\train_00005-of-00020',\n",
      " 'generate_tfrecord\\\\train_00006-of-00020',\n",
      " 'generate_tfrecord\\\\train_00007-of-00020',\n",
      " 'generate_tfrecord\\\\train_00008-of-00020',\n",
      " 'generate_tfrecord\\\\train_00009-of-00020',\n",
      " 'generate_tfrecord\\\\train_00010-of-00020',\n",
      " 'generate_tfrecord\\\\train_00011-of-00020',\n",
      " 'generate_tfrecord\\\\train_00012-of-00020',\n",
      " 'generate_tfrecord\\\\train_00013-of-00020',\n",
      " 'generate_tfrecord\\\\train_00014-of-00020',\n",
      " 'generate_tfrecord\\\\train_00015-of-00020',\n",
      " 'generate_tfrecord\\\\train_00016-of-00020',\n",
      " 'generate_tfrecord\\\\train_00017-of-00020',\n",
      " 'generate_tfrecord\\\\train_00018-of-00020',\n",
      " 'generate_tfrecord\\\\train_00019-of-00020']\n",
      "['generate_tfrecord\\\\valid_00000-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00001-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00002-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00003-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00004-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00005-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00006-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00007-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00008-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00009-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00010-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00011-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00012-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00013-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00014-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00015-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00016-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00017-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00018-of-00020',\n",
      " 'generate_tfrecord\\\\valid_00019-of-00020']\n",
      "['generate_tfrecord\\\\test_00000-of-00020',\n",
      " 'generate_tfrecord\\\\test_00001-of-00020',\n",
      " 'generate_tfrecord\\\\test_00002-of-00020',\n",
      " 'generate_tfrecord\\\\test_00003-of-00020',\n",
      " 'generate_tfrecord\\\\test_00004-of-00020',\n",
      " 'generate_tfrecord\\\\test_00005-of-00020',\n",
      " 'generate_tfrecord\\\\test_00006-of-00020',\n",
      " 'generate_tfrecord\\\\test_00007-of-00020',\n",
      " 'generate_tfrecord\\\\test_00008-of-00020',\n",
      " 'generate_tfrecord\\\\test_00009-of-00020',\n",
      " 'generate_tfrecord\\\\test_00010-of-00020',\n",
      " 'generate_tfrecord\\\\test_00011-of-00020',\n",
      " 'generate_tfrecord\\\\test_00012-of-00020',\n",
      " 'generate_tfrecord\\\\test_00013-of-00020',\n",
      " 'generate_tfrecord\\\\test_00014-of-00020',\n",
      " 'generate_tfrecord\\\\test_00015-of-00020',\n",
      " 'generate_tfrecord\\\\test_00016-of-00020',\n",
      " 'generate_tfrecord\\\\test_00017-of-00020',\n",
      " 'generate_tfrecord\\\\test_00018-of-00020',\n",
      " 'generate_tfrecord\\\\test_00019-of-00020']\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(train_tfrecord_filenames)\n",
    "pprint.pprint(valid_tfrecord_filenames)\n",
    "pprint.pprint(test_tfrecord_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.读取tfrecoreds内容\n",
    "except_feature = {\n",
    "    \"input_features\": tf.io.FixedLenFeature([8], dtype=tf.float32),\n",
    "    \"label\": tf.io.FixedLenFeature([1], dtype=tf.float32),\n",
    "}\n",
    "\n",
    "def parse_example(example):\n",
    "    example = tf.io.parse_single_example(example, except_feature)\n",
    "    features = example[\"input_features\"]\n",
    "    label = example[\"label\"]\n",
    "    \n",
    "    return features, label\n",
    "\n",
    "def tfrecords_to_dataset(filenames, batchsize):\n",
    "    # a.读入文件名\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    \n",
    "    # b.读入文件中的数据  TFRecordDataset\n",
    "    dataset = dataset.interleave(lambda file: tf.data.TFRecordDataset(file), cycle_length = 5)\n",
    "    \n",
    "    # c.对文件中的数据转换\n",
    "    dataset = dataset.map(parse_example)\n",
    "    \n",
    "    # d.生成batch_size的文件\n",
    "    dataset = dataset.batch(batchsize)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_train_dataset = tfrecords_to_dataset(train_tfrecord_filenames, 32)\n",
    "tfrecords_valid_dataset = tfrecords_to_dataset(valid_tfrecord_filenames, 32)\n",
    "tfrecords_test_dataset = tfrecords_to_dataset(test_tfrecord_filenames, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "348/348 [==============================] - 2s 7ms/step - loss: 2.7435 - val_loss: 1.1345\n",
      "Epoch 2/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.9313 - val_loss: 0.7737\n",
      "Epoch 3/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.7165 - val_loss: 0.6874\n",
      "Epoch 4/100\n",
      "348/348 [==============================] - 4s 11ms/step - loss: 0.6528 - val_loss: 0.6482\n",
      "Epoch 5/100\n",
      "348/348 [==============================] - 3s 10ms/step - loss: 0.6293 - val_loss: 0.6249\n",
      "Epoch 6/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.5876 - val_loss: 0.6110\n",
      "Epoch 7/100\n",
      "348/348 [==============================] - 4s 12ms/step - loss: 0.5810 - val_loss: 0.5937\n",
      "Epoch 8/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 0.5415 - val_loss: 0.5778\n",
      "Epoch 9/100\n",
      "348/348 [==============================] - 2s 7ms/step - loss: 0.5595 - val_loss: 0.5619\n",
      "Epoch 10/100\n",
      "348/348 [==============================] - 2s 7ms/step - loss: 0.5106 - val_loss: 0.5513\n",
      "Epoch 11/100\n",
      "348/348 [==============================] - 3s 10ms/step - loss: 0.5312 - val_loss: 0.5387\n",
      "Epoch 12/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 0.4983 - val_loss: 0.5306\n",
      "Epoch 13/100\n",
      "348/348 [==============================] - 3s 8ms/step - loss: 0.4892 - val_loss: 0.5224\n",
      "Epoch 14/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.4847 - val_loss: 0.5140\n",
      "Epoch 15/100\n",
      "348/348 [==============================] - 2s 7ms/step - loss: 0.4883 - val_loss: 0.5107\n",
      "Epoch 16/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.4618 - val_loss: 0.5019\n",
      "Epoch 17/100\n",
      "348/348 [==============================] - 4s 12ms/step - loss: 0.4850 - val_loss: 0.4993\n",
      "Epoch 18/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 0.4582 - val_loss: 0.4963\n",
      "Epoch 19/100\n",
      "348/348 [==============================] - 4s 11ms/step - loss: 0.4617 - val_loss: 0.4890\n",
      "Epoch 20/100\n",
      "348/348 [==============================] - 3s 7ms/step - loss: 0.4547 - val_loss: 0.4868\n",
      "Epoch 21/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.4551 - val_loss: 0.4840\n",
      "Epoch 22/100\n",
      "348/348 [==============================] - 3s 10ms/step - loss: 0.4420 - val_loss: 0.4803\n",
      "Epoch 23/100\n",
      "348/348 [==============================] - 3s 10ms/step - loss: 0.4429 - val_loss: 0.4762\n",
      "Epoch 24/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.4432 - val_loss: 0.4759\n",
      "Epoch 25/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.4362 - val_loss: 0.4726\n",
      "Epoch 26/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.4358 - val_loss: 0.4688\n",
      "Epoch 27/100\n",
      "348/348 [==============================] - 3s 8ms/step - loss: 0.4335 - val_loss: 0.4668\n",
      "Epoch 28/100\n",
      "348/348 [==============================] - 2s 7ms/step - loss: 0.4307 - val_loss: 0.4645\n",
      "Epoch 29/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 0.4306 - val_loss: 0.4622\n",
      "Epoch 30/100\n",
      "348/348 [==============================] - 3s 8ms/step - loss: 0.4239 - val_loss: 0.4607\n",
      "Epoch 31/100\n",
      "348/348 [==============================] - 1s 4ms/step - loss: 0.4261 - val_loss: 0.4591\n",
      "Epoch 32/100\n",
      "348/348 [==============================] - 2s 7ms/step - loss: 0.4194 - val_loss: 0.4559\n",
      "Epoch 33/100\n",
      "348/348 [==============================] - 4s 12ms/step - loss: 0.4221 - val_loss: 0.4541\n",
      "Epoch 34/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 0.4083 - val_loss: 0.4494\n",
      "Epoch 35/100\n",
      "348/348 [==============================] - 2s 7ms/step - loss: 0.4211 - val_loss: 0.4484\n",
      "Epoch 36/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 0.4073 - val_loss: 0.4458\n",
      "Epoch 37/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.4232 - val_loss: 0.4452\n",
      "Epoch 38/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.4004 - val_loss: 0.4418\n",
      "Epoch 39/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 0.4092 - val_loss: 0.4404\n",
      "Epoch 40/100\n",
      "348/348 [==============================] - 2s 5ms/step - loss: 0.4018 - val_loss: 0.4388\n",
      "Epoch 41/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.4068 - val_loss: 0.4352\n",
      "Epoch 42/100\n",
      "348/348 [==============================] - 3s 8ms/step - loss: 0.4022 - val_loss: 0.4346\n",
      "Epoch 43/100\n",
      "348/348 [==============================] - 4s 11ms/step - loss: 0.3954 - val_loss: 0.4314\n",
      "Epoch 44/100\n",
      "348/348 [==============================] - 2s 6ms/step - loss: 0.4096 - val_loss: 0.4310\n",
      "Epoch 45/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 0.3784 - val_loss: 0.4274\n",
      "Epoch 46/100\n",
      "348/348 [==============================] - 3s 9ms/step - loss: 0.4070 - val_loss: 0.4276\n",
      "Epoch 47/100\n",
      "348/348 [==============================] - 2s 7ms/step - loss: 0.3827 - val_loss: 0.4248\n",
      "Epoch 48/100\n",
      "348/348 [==============================] - 3s 7ms/step - loss: 0.3970 - val_loss: 0.4236\n"
     ]
    }
   ],
   "source": [
    "# 8.训练模型\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=[8]),\n",
    "    keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.SGD(0.001))\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=5, min_delta=1e-2)]\n",
    "\n",
    "batch_size= 32\n",
    "history = model.fit(tfrecords_train_dataset.repeat(),   # 加上repeat\n",
    "                    validation_data = tfrecords_valid_dataset,\n",
    "                    steps_per_epoch = 11160 // batch_size,\n",
    "                    validation_steps = 3870 // batch_size,\n",
    "                    epochs = 100,\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 1s 7ms/step - loss: 0.3894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3894062340259552"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9.评价模型\n",
    "model.evaluate(tfrecords_test_dataset.repeat(), steps = 5160 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
